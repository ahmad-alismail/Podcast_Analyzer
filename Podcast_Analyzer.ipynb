{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Kabanosk/whisper-website\n",
    "https://github.com/openai/whisper/discussions/264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yt_dlp\n",
    "import unzip\n",
    "\n",
    "# increase column width\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Audio and Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_URL = \"https://youtu.be/DgTjSrrf6GQ\"\n",
    "AUDIO_FILE_NAME = \"./data/Lex_Podcast.mp3\"\n",
    "AUDIO_QUALITY = 5 # 0 best - 10 worst (default 5)\n",
    "AUDIO_FORMAT = \"mp3\"\n",
    "FFMPEG_LOCATION = \"ffmpeg-master-latest-win64-gpl/bin\"\n",
    "SUBTITLE_LANGUAGE = \"en.*\"\n",
    "TRANSCRIPT_FILE_NAME = \"./data/transcript.txt\"\n",
    "SUBTITLE_FORMAT = \"srt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ffmpeg...\n",
      "Unzipping...\n",
      "Removing zip file...\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "FFMPEG_URL = 'https://github.com/yt-dlp/FFmpeg-Builds/releases/download/latest/ffmpeg-master-latest-win64-gpl.zip'\n",
    "ZIP_PATH = './ffmpeg.zip'\n",
    "EXTRACT_DIR = './'\n",
    "\n",
    "if not os.path.exists(ZIP_PATH):\n",
    "    print('Downloading ffmpeg...')\n",
    "    wget.download(FFMPEG_URL, ZIP_PATH)\n",
    "\n",
    "    print('Unzipping...') \n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(EXTRACT_DIR)\n",
    "    \n",
    "    print('Removing zip file...')\n",
    "    os.remove(ZIP_PATH)\n",
    "\n",
    "else:\n",
    "    print('Already downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yt-dlp -xv --ffmpeg-location ffmpeg-master-latest-win64-gpl/bin --audio-format mp3  -o data/Lex_Podcast -- {\"https://youtu.be/DEu24V8vfb8\"}\n",
    "#!yt-dlp -xv --ffmpeg-location {FFMPEG_LOCATION} --audio-format {AUDIO_FORMAT}  -o {AUDIO_FILE_NAME} -- {\"https://youtu.be/DEu24V8vfb8\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "audio_file = open(\"audio.mp3\", \"rb\")\n",
    "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file, response_format=SUBTITLE_FORMAT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer Inputs\n",
    "By default, the Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost.\n",
    "\n",
    "One way to handle this is to use the [PyDub open source Python package](https://github.com/jiaaro/pydub) to split the audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "song = AudioSegment.from_mp3(\"good_morning.mp3\")\n",
    "\n",
    "# PyDub handles time in milliseconds\n",
    "ten_minutes = 10 * 60 * 1000\n",
    "\n",
    "first_10_minutes = song[:ten_minutes]\n",
    "\n",
    "first_10_minutes.export(\"good_morning_10.mp3\", format=\"mp3\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting\n",
    "* Check out [OpenAI](https://platform.openai.com/docs/guides/speech-to-text/prompting)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtu.be/DgTjSrrf6GQ\n",
      "[youtube] DgTjSrrf6GQ: Downloading webpage\n",
      "[youtube] DgTjSrrf6GQ: Downloading android player API JSON\n",
      "[info] DgTjSrrf6GQ: Downloading subtitles: en-orig, en, en-en-ehkg1hFWq8A\n",
      "[info] DgTjSrrf6GQ: Downloading 1 format(s): 22\n",
      "[info] Writing video subtitles to: transcript.txt.en-orig.vtt\n",
      "[download] Destination: transcript.txt.en-orig.vtt\n",
      "\n",
      "[download]    1.00KiB at  909.63KiB/s (00:00:00)\n",
      "[download]    3.00KiB at    1.45MiB/s (00:00:00)\n",
      "[download]    7.00KiB at    1.32MiB/s (00:00:00)\n",
      "[download]   15.00KiB at    1.17MiB/s (00:00:00)\n",
      "[download]   31.00KiB at  729.35KiB/s (00:00:00)\n",
      "[download]   63.00KiB at  978.15KiB/s (00:00:00)\n",
      "[download]  127.00KiB at    1.33MiB/s (00:00:00)\n",
      "[download]  255.00KiB at    2.07MiB/s (00:00:00)\n",
      "[download]  511.00KiB at    3.35MiB/s (00:00:00)\n",
      "[download]  816.60KiB at    4.29MiB/s (00:00:00)\n",
      "[download] 100% of  816.60KiB in 00:00:00 at 1.88MiB/s\n",
      "[info] Writing video subtitles to: transcript.txt.en.vtt\n",
      "[download] Destination: transcript.txt.en.vtt\n",
      "\n",
      "[download]    1.00KiB at  965.54KiB/s (00:00:00)\n",
      "[download]    3.00KiB at    1.47MiB/s (00:00:00)\n",
      "[download]    7.00KiB at    2.28MiB/s (00:00:00)\n",
      "[download]   15.00KiB at    1.48MiB/s (00:00:00)\n",
      "[download]   31.00KiB at  917.91KiB/s (00:00:00)\n",
      "[download]   63.00KiB at    1.05MiB/s (00:00:00)\n",
      "[download]  127.00KiB at    1.50MiB/s (00:00:00)\n",
      "[download]  255.00KiB at    2.26MiB/s (00:00:00)\n",
      "[download]  511.00KiB at    3.53MiB/s (00:00:00)\n",
      "[download]  816.60KiB at    4.41MiB/s (00:00:00)\n",
      "[download] 100% of  816.60KiB in 00:00:00 at 2.06MiB/s\n",
      "[info] Writing video subtitles to: transcript.txt.en-en-ehkg1hFWq8A.vtt\n",
      "[download] Destination: transcript.txt.en-en-ehkg1hFWq8A.vtt\n",
      "\n",
      "[download]    1.00KiB at  906.88KiB/s (00:00:00)\n",
      "[download]    3.00KiB at    2.66MiB/s (00:00:00)\n",
      "[download]    7.00KiB at    2.03MiB/s (00:00:00)\n",
      "[download]   15.00KiB at    1.34MiB/s (00:00:00)\n",
      "[download]   31.00KiB at  704.03KiB/s (00:00:00)\n",
      "[download]   63.00KiB at  921.55KiB/s (00:00:00)\n",
      "[download]  127.00KiB at    1.28MiB/s (00:00:00)\n",
      "[download]  150.19KiB at    1.42MiB/s (00:00:00)\n",
      "[download] 100% of  150.19KiB in 00:00:00 at 579.33KiB/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: No subtitle format found matching \"srt\" for language en-orig, using vtt\n",
      "WARNING: No subtitle format found matching \"srt\" for language en, using vtt\n",
      "WARNING: No subtitle format found matching \"srt\" for language en-en-ehkg1hFWq8A, using vtt\n"
     ]
    }
   ],
   "source": [
    "# Download the transcript with yt-dlp\n",
    "!yt-dlp --write-auto-sub --skip-download --sub-format {SUBTITLE_FORMAT} --sub-lang {SUBTITLE_LANGUAGE} --output {TRANSCRIPT_FILE_NAME} -- {VIDEO_URL}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Let's change the name of the raw caption files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a clean list of podcast titles\n",
    "import re \n",
    "\n",
    "def clean_titles(title):\n",
    "    title = re.sub(r'\\[(.*?)\\]\\((.*?)\\)', r'\\1', title)\n",
    "    title = re.sub(r'\\|.*?\\d+', '', title)\n",
    "    title = title.rstrip().replace(' ', '_').replace(':', '_').replace('&','and').lower()\n",
    "    title = re.sub(r'[^a-zA-Z0-9_]', '', title)\n",
    "    return title\n",
    "\n",
    "with open('./data/Lexicap.md', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "titles = text.split('\\n')\n",
    "titles = [clean_titles(title) for title in titles if title != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the transcript files\n",
    "import os\n",
    "\n",
    "TRANSCRIPT_PATH = \"./data/transcripts/\"\n",
    "FILE_EXTENSION = '.vtt'\n",
    "\n",
    "for org_filename, line_idx in zip(os.listdir(path=TRANSCRIPT_PATH), titles):\n",
    "    if org_filename.endswith(FILE_EXTENSION):\n",
    "        print(org_filename)\n",
    "        # rename file \n",
    "        new_filename = f\"{line_idx}{FILE_EXTENSION}\"\n",
    "        os.rename(f\"{TRANSCRIPT_PATH}{org_filename}\", f\"{TRANSCRIPT_PATH}{new_filename}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSCRIPT_PATH = \"./data/transcripts/\"\n",
    "FILE_EXTENSION = '.vtt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new transcript file with timestamp and text\n",
    "TRANSCRIPT_FILE_NAME = \"45_michio_kaku__future_of_humans_aliens_space_travel_and_physics.vtt\"\n",
    "NEW_TRANSCRIPT_FILE_NAME = \"45_michio_kaku__future_of_humans_aliens_space_travel_and_physics.csv\"\n",
    "\n",
    "\n",
    "with open(f\"{TRANSCRIPT_PATH}{TRANSCRIPT_FILE_NAME}\") as oldfile, open(f\"{NEW_TRANSCRIPT_FILE_NAME}\", 'w') as newfile:\n",
    "    old_lines = oldfile.read().split('\\n')\n",
    "    clean_lines = [line for line in old_lines if line not in ['', 'WEBVTT']]\n",
    "\n",
    "    for line_idx in range(0, len(clean_lines)-1, 2):\n",
    "         timestamp = clean_lines[line_idx].split('-->')[0].strip()\n",
    "         text = clean_lines[line_idx+1].rstrip()\n",
    "         new_line = f\"{timestamp};{text}\\n\"\n",
    "         newfile.write(new_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamps</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00.000</td>\n",
       "      <td>The following is a conversation with Michio Kaku.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:02.800</td>\n",
       "      <td>He's a theoretical physicist, futurist,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:05.120</td>\n",
       "      <td>and professor at the City College of New York.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:08.360</td>\n",
       "      <td>He's the author of many fascinating books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:10.760</td>\n",
       "      <td>that explore the nature of our reality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00:12.840</td>\n",
       "      <td>and the future of our civilization.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00:15.520</td>\n",
       "      <td>They include Einstein's Cosmos, Physics of the Impossible,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00:19.200</td>\n",
       "      <td>Future of the Mind, Parallel Worlds,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00:21.600</td>\n",
       "      <td>and his latest, The Future of Humanity,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00:24.240</td>\n",
       "      <td>Terraforming Mars Interstellar Travel,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>00:26.640</td>\n",
       "      <td>Immortality, and Our Destiny Beyond Earth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00:29.960</td>\n",
       "      <td>I think it's beautiful and important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00:32.960</td>\n",
       "      <td>when a scientific mind can fearlessly explore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00:35.760</td>\n",
       "      <td>through conversation subjects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00:37.600</td>\n",
       "      <td>just outside of our understanding.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00:40.200</td>\n",
       "      <td>That, to me, is where artificial intelligence is today,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00:43.440</td>\n",
       "      <td>just outside of our understanding,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00:45.680</td>\n",
       "      <td>a place we have to reach for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00:47.440</td>\n",
       "      <td>if we're to uncover the mysteries of the human mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00:50.160</td>\n",
       "      <td>and build human level and superhuman level AI systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>00:53.880</td>\n",
       "      <td>that transform our world for the better.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>00:56.640</td>\n",
       "      <td>This is the Artificial Intelligence Podcast.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>00:59.240</td>\n",
       "      <td>If you enjoy it, subscribe on YouTube,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>01:01.600</td>\n",
       "      <td>give it five stars on iTunes, support it on Patreon,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>01:04.640</td>\n",
       "      <td>or simply connect with me on Twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamps                                                         text\n",
       "0   00:00.000            The following is a conversation with Michio Kaku.\n",
       "1   00:02.800                      He's a theoretical physicist, futurist,\n",
       "2   00:05.120               and professor at the City College of New York.\n",
       "3   00:08.360                    He's the author of many fascinating books\n",
       "4   00:10.760                       that explore the nature of our reality\n",
       "5   00:12.840                          and the future of our civilization.\n",
       "6   00:15.520   They include Einstein's Cosmos, Physics of the Impossible,\n",
       "7   00:19.200                         Future of the Mind, Parallel Worlds,\n",
       "8   00:21.600                      and his latest, The Future of Humanity,\n",
       "9   00:24.240                       Terraforming Mars Interstellar Travel,\n",
       "10  00:26.640                   Immortality, and Our Destiny Beyond Earth.\n",
       "11  00:29.960                         I think it's beautiful and important\n",
       "12  00:32.960                when a scientific mind can fearlessly explore\n",
       "13  00:35.760                                through conversation subjects\n",
       "14  00:37.600                           just outside of our understanding.\n",
       "15  00:40.200      That, to me, is where artificial intelligence is today,\n",
       "16  00:43.440                           just outside of our understanding,\n",
       "17  00:45.680                                 a place we have to reach for\n",
       "18  00:47.440          if we're to uncover the mysteries of the human mind\n",
       "19  00:50.160        and build human level and superhuman level AI systems\n",
       "20  00:53.880                     that transform our world for the better.\n",
       "21  00:56.640                 This is the Artificial Intelligence Podcast.\n",
       "22  00:59.240                       If you enjoy it, subscribe on YouTube,\n",
       "23  01:01.600         give it five stars on iTunes, support it on Patreon,\n",
       "24  01:04.640                         or simply connect with me on Twitter"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_df = pd.read_csv(f\"{NEW_TRANSCRIPT_FILE_NAME}\"\n",
    "                            ,sep=';', \n",
    "                            header=None, names=['timestamps', 'text'],\n",
    "                            parse_dates=['timestamps'], date_format='%H:%M:%S.%f')\n",
    "\n",
    "transcript_df.head(25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ideas\n",
    "* Summarization of main topics in the audio\n",
    "* Go to the mentions of the topics\n",
    "* translation to arabic\n",
    "* The app ask me questions about the text (for language learning) and create a discussion\n",
    "* Overall sentiment in the text\n",
    "* Webapp or Mobile app"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since Whisper has not skipped the punctuations in the transcript, we can reconstruct full sentences, ensuring that each sentence is ended with a period `.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the dataframe with full sentences\n",
    "full_transcript_df = pd.DataFrame(columns=transcript_df.columns)\n",
    "\n",
    "for idx, ts, text in transcript_df.itertuples():\n",
    "    while text[-1] != '.':\n",
    "        idx += 1\n",
    "        text += transcript_df.loc[idx]['text']\n",
    "    full_transcript_df = pd.concat([full_transcript_df, pd.DataFrame({'timestamps': ts, 'text': text}, index=[0])], ignore_index=True)\n",
    "\n",
    "# Remove any piece of text if it is included in previous text\n",
    "not_part_of_previous = [True]\n",
    "for i in range(1, len(full_transcript_df)):\n",
    "    not_part_of_previous.append(full_transcript_df['text'][i] not in full_transcript_df['text'][i-1])\n",
    "full_transcript_df = full_transcript_df[not_part_of_previous] \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NER\n",
    "* Add column for persons, orginazations, books, companies, countries, places."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mentioned Books\n",
    "Why identifying book titles can be difficult?\n",
    "* The book may contain persons names which are not authors.\n",
    "* The book titles are difficult to identify as such in general. For example \"the Republic\" might or might not be about the book, and if the only indication the model can use is the capitalization it's probably going to make some errors.\n",
    "\n",
    "To be clear, I think it could work to some extent but it would probably make quite a lot of errors.\n",
    "\n",
    "* On the other hand you could obtain a database of books, for instance from Wikipedia (there might be better resources), and you could use this in two ways:\n",
    "\n",
    "1. Directly identify the books/authors in the documents by simple string matching. I would imagine that even if the coverage of the resource is not perfect, this method would easily catch a majority of occurrences.\n",
    "2. In case the above method is not sufficient, it provides you with some good training data from which you could train a NER model in order collect titles which don't exist in the database. Note that there might be issues due to the unknown books being labelled as negative in the training data, so ideally you would have to go manually through the training data and annotate the remaining cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# %pip install -U spaCy\n",
    "# %pip install 'spacy[transformers]'\n",
    "#!python -m spacy download en_core_web_trf # download best-matching version of specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('transformer',\n",
       "  <spacy_transformers.pipeline_component.Transformer at 0x2b0023d32e0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x2b0023d3340>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x2b00231bed0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x2b07e81e140>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x2b001d89d40>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x2b0035ac0b0>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "# load a pipeline package by name and return nlp object\n",
    "nlp = spacy.load(\"en_core_web_trf\", disable=[\"tok2vec\"]) #disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\", \"ner\"]\n",
    "\n",
    "# check processing pipeline components of nlp object\n",
    "nlp.pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities:  ['Collapse']\n"
     ]
    }
   ],
   "source": [
    "# create a Doc by processing a string of text with the nlp object\n",
    "doc = nlp(\"So I'm with Jared Diamond, you know, in the book Collapse, \\\n",
    "          where he points out studying the collapse of major civilizations, \\\n",
    "          that it often happens right after things appear to never have been better. Hmm.\")\n",
    "\n",
    "\n",
    "# iterate over tokens in a Doc\n",
    "print(\"Entities: \", [e.text for e in doc.ents if e.label_ == 'WORK_OF_ART'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find book related sentences in the transcript\n",
    "book_related_phrases = [\n",
    "    \"book\", \"books\", \"i read\", \n",
    "    \"everyone should read\", \"you should read\", \"he wrote a novel\",\n",
    "    \"i recommend\", \"highly recommend\", \"you must read\", \n",
    "    \"shouldn't miss\", \"top books\", \"best books\", \n",
    "    \"favorite book\", \"my favorite books\", \"book you need to read\",\n",
    "    \"books to read before\", \"essential books\", \"great book for\",\n",
    "    \"worthy read\", \"book of the year\", \"award winning book\"\n",
    "]\n",
    "\n",
    "\n",
    "def contains_book_phrase(text):\n",
    "    return any(phrase in text.lower() for phrase in book_related_phrases)\n",
    "\n",
    "full_transcript_df[\"is_book_related\"] = full_transcript_df[\"text\"].apply(contains_book_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_titles_candidates(text):\n",
    "    doc = nlp(text)\n",
    "    book_titles_candidates = [e.text for e in doc.ents if e.label_ == 'WORK_OF_ART']\n",
    "    return book_titles_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3                               []\n",
       "182                             []\n",
       "341                             []\n",
       "478       [The Future of the Mind]\n",
       "638                             []\n",
       "859                             []\n",
       "940                             []\n",
       "954     [The Theory of Everything]\n",
       "957                             []\n",
       "1149          [Future of Humanity]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "full_transcript_df.query(\"is_book_related == True\")[\"text\"].apply(get_book_titles_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add book candidates to the dataframe\n",
    "full_transcript_df[\"book_candidates\"] = full_transcript_df.apply(lambda x: get_book_titles_candidates(x[\"text\"]) \\\n",
    "                                                                 if x[\"is_book_related\"] else [], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamps</th>\n",
       "      <th>text</th>\n",
       "      <th>is_book_related</th>\n",
       "      <th>book_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:08.360</td>\n",
       "      <td>He's the author of many fascinating books that explore the nature of our reality and the future of our civilization.</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>08:30.920</td>\n",
       "      <td>And Stephen Hawking, for example, even in his last book, even said that this is an argument against the existence of God.</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>16:04.760</td>\n",
       "      <td>If you read the book, the aliens did not have evil intentions toward homo sapiens.</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>22:38.600</td>\n",
       "      <td>I have a book, The Future of the Mind, where I detail some of these breakthroughs.</td>\n",
       "      <td>True</td>\n",
       "      <td>[The Future of the Mind]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>29:54.080</td>\n",
       "      <td>Our ancestors were lucky if they had one line, just one line in a church book, saying the date they were baptized and the date they died.</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>40:07.240</td>\n",
       "      <td>For Isidor Rabi, it was a book about the planets.</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>44:06.200</td>\n",
       "      <td>That desk had a book on it, which was opened.</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>44:43.160</td>\n",
       "      <td>And then over the years, I found out the guy had a name, Albert Einstein, and that book was The Theory of Everything.</td>\n",
       "      <td>True</td>\n",
       "      <td>[The Theory of Everything]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>44:53.080</td>\n",
       "      <td>Well, today I can read that book.</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>53:36.600</td>\n",
       "      <td>And in my book, Future of Humanity, I even speculate beyond that, that by the end of this century, we'll probably have the first starships.</td>\n",
       "      <td>True</td>\n",
       "      <td>[Future of Humanity]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     timestamps   \n",
       "3     00:08.360  \\\n",
       "182   08:30.920   \n",
       "341   16:04.760   \n",
       "478   22:38.600   \n",
       "638   29:54.080   \n",
       "859   40:07.240   \n",
       "940   44:06.200   \n",
       "954   44:43.160   \n",
       "957   44:53.080   \n",
       "1149  53:36.600   \n",
       "\n",
       "                                                                                                                                              text   \n",
       "3                             He's the author of many fascinating books that explore the nature of our reality and the future of our civilization.  \\\n",
       "182                      And Stephen Hawking, for example, even in his last book, even said that this is an argument against the existence of God.   \n",
       "341                                                             If you read the book, the aliens did not have evil intentions toward homo sapiens.   \n",
       "478                                                             I have a book, The Future of the Mind, where I detail some of these breakthroughs.   \n",
       "638      Our ancestors were lucky if they had one line, just one line in a church book, saying the date they were baptized and the date they died.   \n",
       "859                                                                                              For Isidor Rabi, it was a book about the planets.   \n",
       "940                                                                                                  That desk had a book on it, which was opened.   \n",
       "954                          And then over the years, I found out the guy had a name, Albert Einstein, and that book was The Theory of Everything.   \n",
       "957                                                                                                              Well, today I can read that book.   \n",
       "1149   And in my book, Future of Humanity, I even speculate beyond that, that by the end of this century, we'll probably have the first starships.   \n",
       "\n",
       "      is_book_related             book_candidates  \n",
       "3                True                          []  \n",
       "182              True                          []  \n",
       "341              True                          []  \n",
       "478              True    [The Future of the Mind]  \n",
       "638              True                          []  \n",
       "859              True                          []  \n",
       "940              True                          []  \n",
       "954              True  [The Theory of Everything]  \n",
       "957              True                          []  \n",
       "1149             True        [Future of Humanity]  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_transcript_df.query(\"is_book_related == True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'The Future of Humanity',\n",
       " 'subtitle': 'Terraforming Mars, Interstellar Travel, Immortality, and Our Destiny Beyond',\n",
       " 'authors': ['Michio Kaku'],\n",
       " 'publisher': 'Penguin UK',\n",
       " 'publishedDate': '2018-02-28',\n",
       " 'description': 'Human civilization is on the verge of spreading beyond Earth. More than a possibility, it is becoming a necessity: whether our hand is forced by climate change and resource depletion or whether future catastrophes compel us to abandon Earth, one day we will make our homes among the stars. World-renowned physicist and futurist Michio Kaku explores in rich, accessible detail how humanity might gradually develop a sustainable civilization in outer space. With his trademark storytelling verve, Kaku shows us how science fiction is becoming reality: mind-boggling developments in robotics, nanotechnology, and biotechnology could enable us to build habitable cities on Mars; nearby stars might be reached by microscopic spaceships sailing through space on laser beams; and technology might one day allow us to transcend our physical bodies entirely. With irrepressible enthusiasm and wonder, Dr. Kaku takes readers on a fascinating journey to a future in which humanity could finally fulfil its long-awaited destiny among the stars - and perhaps even achieve immortality.',\n",
       " 'industryIdentifiers': [{'type': 'ISBN_13', 'identifier': '9780141986050'},\n",
       "  {'type': 'ISBN_10', 'identifier': '0141986050'}],\n",
       " 'readingModes': {'text': True, 'image': False},\n",
       " 'pageCount': 336,\n",
       " 'printType': 'BOOK',\n",
       " 'categories': ['Science'],\n",
       " 'averageRating': 3.5,\n",
       " 'ratingsCount': 9,\n",
       " 'maturityRating': 'NOT_MATURE',\n",
       " 'allowAnonLogging': True,\n",
       " 'contentVersion': '1.6.7.0.preview.2',\n",
       " 'panelizationSummary': {'containsEpubBubbles': False,\n",
       "  'containsImageBubbles': False},\n",
       " 'imageLinks': {'smallThumbnail': 'http://books.google.com/books/content?id=fSw6DwAAQBAJ&printsec=frontcover&img=1&zoom=5&edge=curl&source=gbs_api',\n",
       "  'thumbnail': 'http://books.google.com/books/content?id=fSw6DwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api'},\n",
       " 'language': 'en',\n",
       " 'previewLink': 'http://books.google.de/books?id=fSw6DwAAQBAJ&printsec=frontcover&dq=Future+of+Humanity&hl=&cd=1&source=gbs_api',\n",
       " 'infoLink': 'https://play.google.com/store/books/details?id=fSw6DwAAQBAJ&source=gbs_api',\n",
       " 'canonicalVolumeLink': 'https://play.google.com/store/books/details?id=fSw6DwAAQBAJ'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get book info from Google Books API \n",
    "import requests\n",
    "\n",
    "def get_book_info(title):\n",
    "    response = requests.get(f\"https://www.googleapis.com/books/v1/volumes?q={title}\")\n",
    "    data = response.json()\n",
    "    if 'items' in data:\n",
    "        # Just return the first book found\n",
    "        book = data['items'][0]['volumeInfo']\n",
    "        return book\n",
    "\n",
    "get_book_info(\"Future of Humanity\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Other Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "\n",
    "entity_types = ['PERSON', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'DATE', 'TIME', 'MONEY']\n",
    "\n",
    "def extract_entities(doc, include_types=entity_types, sep='_'):\n",
    "    ents = textacy.extract.entities(doc, \n",
    "             include_types=include_types, \n",
    "             exclude_types=None, \n",
    "             drop_determiners=True, \n",
    "             min_freq=1)\n",
    "    \n",
    "    return [sep.join([token.lemma_ for token in e])+'/'+e.label_ for e in ents]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When processing large volumes of text, it is recommended to use spaCys batch processing for a significant performance gain. The function `nlp.pipeline` takes an iterable of texts, processes them internally as batch, and yields a list fo processed Doc objects in the same order as the input data.\n",
    "* To use `nlp.pipeline`, we first have to define a batch size. Then we can loop over the batches and call `nlp.pipe`. In the inner loop we extract the features from the processed doc and write the values back into the a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:56<00:00,  4.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# Extract entities from the transcript\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 50\n",
    "batches = np.ceil(len(full_transcript_df) / batch_size).astype(int)\n",
    "\n",
    "named_entities = []\n",
    "\n",
    "# loop over batches, step size is equal to batch size\n",
    "for i in tqdm(range(0, len(full_transcript_df), batch_size), total=batches):\n",
    "    docs = nlp.pipe(full_transcript_df['text'][i:i+batch_size])\n",
    "    \n",
    "    for doc in docs:\n",
    "        named_entities.append(extract_entities(doc)) \n",
    "\n",
    "full_transcript_df['named_entities'] = named_entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent_type in entity_types:\n",
    "    full_transcript_df[ent_type.lower()] = full_transcript_df[\"named_entities\"].apply(lambda x: \\\n",
    "                                                                            [e.split('/')[0] for e in x if e.split('/')[1] == ent_type])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty lists from the dataframe\n",
    "\n",
    "# Check if a column contains empty lists\n",
    "def contains_empty_list(col_name):\n",
    "    return full_transcript_df[col_name].apply(lambda x: x == []).any()\n",
    "\n",
    "# Find columns with empty lists\n",
    "cols_to_clean = [col_name for col_name in full_transcript_df.columns if contains_empty_list(col_name)]\n",
    "\n",
    "for ent_type in cols_to_clean:\n",
    "    full_transcript_df[ent_type] = full_transcript_df[ent_type].apply(lambda x: x if len(x) > 0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "\n",
    "def count_words(df, col_name, preprocess=None, min_freq=2):\n",
    "    counter = Counter()\n",
    "    \n",
    "    # Check if preprocess is None before calling df[col_name].map()\n",
    "    if preprocess is None:\n",
    "        df[col_name].map(counter.update)\n",
    "    else:\n",
    "        df[col_name].map(lambda doc: counter.update(preprocess(doc)))\n",
    "\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = col_name\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Einstein</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tithonus</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albert_Einstein</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aurora</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michio_Kaku</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kardashev</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stephen_Hawking</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Steven_Weinberg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Galileo</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shakespeare</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elon_Musk</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charlie_Chaplin</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zeus</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dick</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 freq\n",
       "person               \n",
       "Einstein            7\n",
       "Tithonus            5\n",
       "Albert_Einstein     3\n",
       "Aurora              3\n",
       "Michio_Kaku         2\n",
       "Kardashev           2\n",
       "Stephen_Hawking     2\n",
       "Steven_Weinberg     2\n",
       "Galileo             2\n",
       "Shakespeare         2\n",
       "Elon_Musk           2\n",
       "Charlie_Chaplin     2\n",
       "Zeus                2\n",
       "Dick                2"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_words(full_transcript_df, 'person', min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
