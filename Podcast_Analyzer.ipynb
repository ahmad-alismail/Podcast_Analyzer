{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Kabanosk/whisper-website\n",
    "https://github.com/openai/whisper/discussions/264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yt_dlp\n",
    "import unzip\n",
    "\n",
    "# increase column width\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Audio and Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_URL = \"https://youtu.be/DgTjSrrf6GQ\"\n",
    "AUDIO_FILE_NAME = \"./data/Lex_Podcast.mp3\"\n",
    "AUDIO_QUALITY = 5 # 0 best - 10 worst (default 5)\n",
    "AUDIO_FORMAT = \"mp3\"\n",
    "FFMPEG_LOCATION = \"ffmpeg-master-latest-win64-gpl/bin\"\n",
    "SUBTITLE_LANGUAGE = \"en.*\"\n",
    "TRANSCRIPT_FILE_NAME = \"./data/transcript.txt\"\n",
    "SUBTITLE_FORMAT = \"srt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ffmpeg...\n",
      "Unzipping...\n",
      "Removing zip file...\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "FFMPEG_URL = 'https://github.com/yt-dlp/FFmpeg-Builds/releases/download/latest/ffmpeg-master-latest-win64-gpl.zip'\n",
    "ZIP_PATH = './ffmpeg.zip'\n",
    "EXTRACT_DIR = './'\n",
    "\n",
    "if not os.path.exists(ZIP_PATH):\n",
    "    print('Downloading ffmpeg...')\n",
    "    wget.download(FFMPEG_URL, ZIP_PATH)\n",
    "\n",
    "    print('Unzipping...') \n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(EXTRACT_DIR)\n",
    "    \n",
    "    print('Removing zip file...')\n",
    "    os.remove(ZIP_PATH)\n",
    "\n",
    "else:\n",
    "    print('Already downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yt-dlp -xv --ffmpeg-location ffmpeg-master-latest-win64-gpl/bin --audio-format mp3  -o data/Lex_Podcast -- {\"https://youtu.be/DEu24V8vfb8\"}\n",
    "#!yt-dlp -xv --ffmpeg-location {FFMPEG_LOCATION} --audio-format {AUDIO_FORMAT}  -o {AUDIO_FILE_NAME} -- {\"https://youtu.be/DEu24V8vfb8\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "audio_file = open(\"audio.mp3\", \"rb\")\n",
    "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file, response_format=SUBTITLE_FORMAT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer Inputs\n",
    "By default, the Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost.\n",
    "\n",
    "One way to handle this is to use the [PyDub open source Python package](https://github.com/jiaaro/pydub) to split the audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "song = AudioSegment.from_mp3(\"good_morning.mp3\")\n",
    "\n",
    "# PyDub handles time in milliseconds\n",
    "ten_minutes = 10 * 60 * 1000\n",
    "\n",
    "first_10_minutes = song[:ten_minutes]\n",
    "\n",
    "first_10_minutes.export(\"good_morning_10.mp3\", format=\"mp3\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting\n",
    "* Check out [OpenAI](https://platform.openai.com/docs/guides/speech-to-text/prompting)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtu.be/DgTjSrrf6GQ\n",
      "[youtube] DgTjSrrf6GQ: Downloading webpage\n",
      "[youtube] DgTjSrrf6GQ: Downloading android player API JSON\n",
      "[info] DgTjSrrf6GQ: Downloading subtitles: en-orig, en, en-en-ehkg1hFWq8A\n",
      "[info] DgTjSrrf6GQ: Downloading 1 format(s): 22\n",
      "[info] Writing video subtitles to: transcript.txt.en-orig.vtt\n",
      "[download] Destination: transcript.txt.en-orig.vtt\n",
      "\n",
      "[download]    1.00KiB at  909.63KiB/s (00:00:00)\n",
      "[download]    3.00KiB at    1.45MiB/s (00:00:00)\n",
      "[download]    7.00KiB at    1.32MiB/s (00:00:00)\n",
      "[download]   15.00KiB at    1.17MiB/s (00:00:00)\n",
      "[download]   31.00KiB at  729.35KiB/s (00:00:00)\n",
      "[download]   63.00KiB at  978.15KiB/s (00:00:00)\n",
      "[download]  127.00KiB at    1.33MiB/s (00:00:00)\n",
      "[download]  255.00KiB at    2.07MiB/s (00:00:00)\n",
      "[download]  511.00KiB at    3.35MiB/s (00:00:00)\n",
      "[download]  816.60KiB at    4.29MiB/s (00:00:00)\n",
      "[download] 100% of  816.60KiB in 00:00:00 at 1.88MiB/s\n",
      "[info] Writing video subtitles to: transcript.txt.en.vtt\n",
      "[download] Destination: transcript.txt.en.vtt\n",
      "\n",
      "[download]    1.00KiB at  965.54KiB/s (00:00:00)\n",
      "[download]    3.00KiB at    1.47MiB/s (00:00:00)\n",
      "[download]    7.00KiB at    2.28MiB/s (00:00:00)\n",
      "[download]   15.00KiB at    1.48MiB/s (00:00:00)\n",
      "[download]   31.00KiB at  917.91KiB/s (00:00:00)\n",
      "[download]   63.00KiB at    1.05MiB/s (00:00:00)\n",
      "[download]  127.00KiB at    1.50MiB/s (00:00:00)\n",
      "[download]  255.00KiB at    2.26MiB/s (00:00:00)\n",
      "[download]  511.00KiB at    3.53MiB/s (00:00:00)\n",
      "[download]  816.60KiB at    4.41MiB/s (00:00:00)\n",
      "[download] 100% of  816.60KiB in 00:00:00 at 2.06MiB/s\n",
      "[info] Writing video subtitles to: transcript.txt.en-en-ehkg1hFWq8A.vtt\n",
      "[download] Destination: transcript.txt.en-en-ehkg1hFWq8A.vtt\n",
      "\n",
      "[download]    1.00KiB at  906.88KiB/s (00:00:00)\n",
      "[download]    3.00KiB at    2.66MiB/s (00:00:00)\n",
      "[download]    7.00KiB at    2.03MiB/s (00:00:00)\n",
      "[download]   15.00KiB at    1.34MiB/s (00:00:00)\n",
      "[download]   31.00KiB at  704.03KiB/s (00:00:00)\n",
      "[download]   63.00KiB at  921.55KiB/s (00:00:00)\n",
      "[download]  127.00KiB at    1.28MiB/s (00:00:00)\n",
      "[download]  150.19KiB at    1.42MiB/s (00:00:00)\n",
      "[download] 100% of  150.19KiB in 00:00:00 at 579.33KiB/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: No subtitle format found matching \"srt\" for language en-orig, using vtt\n",
      "WARNING: No subtitle format found matching \"srt\" for language en, using vtt\n",
      "WARNING: No subtitle format found matching \"srt\" for language en-en-ehkg1hFWq8A, using vtt\n"
     ]
    }
   ],
   "source": [
    "# Download the transcript with yt-dlp\n",
    "!yt-dlp --write-auto-sub --skip-download --sub-format {SUBTITLE_FORMAT} --sub-lang {SUBTITLE_LANGUAGE} --output {TRANSCRIPT_FILE_NAME} -- {VIDEO_URL}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Let's change the name of the raw caption files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a clean list of podcast titles\n",
    "import re \n",
    "\n",
    "def clean_titles(title):\n",
    "    title = re.sub(r'\\[(.*?)\\]\\((.*?)\\)', r'\\1', title)\n",
    "    title = re.sub(r'\\|.*?\\d+', '', title)\n",
    "    title = title.rstrip().replace(' ', '_').replace(':', '_').replace('&','and').lower()\n",
    "    title = re.sub(r'[^a-zA-Z0-9_]', '', title)\n",
    "    return title\n",
    "\n",
    "with open('./data/Lexicap.md', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "titles = text.split('\\n')\n",
    "titles = [clean_titles(title) for title in titles if title != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the transcript files\n",
    "import os\n",
    "\n",
    "TRANSCRIPT_PATH = \"./data/transcripts/\"\n",
    "FILE_EXTENSION = '.vtt'\n",
    "\n",
    "for org_filename, line_idx in zip(os.listdir(path=TRANSCRIPT_PATH), titles):\n",
    "    if org_filename.endswith(FILE_EXTENSION):\n",
    "        print(org_filename)\n",
    "        # rename file \n",
    "        new_filename = f\"{line_idx}{FILE_EXTENSION}\"\n",
    "        os.rename(f\"{TRANSCRIPT_PATH}{org_filename}\", f\"{TRANSCRIPT_PATH}{new_filename}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSCRIPT_PATH = \"./data/transcripts/\"\n",
    "FILE_EXTENSION = '.vtt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new transcript file with timestamp and text\n",
    "TRANSCRIPT_FILE_NAME = \"45_michio_kaku__future_of_humans_aliens_space_travel_and_physics.vtt\"\n",
    "NEW_TRANSCRIPT_FILE_NAME = \"45_michio_kaku__future_of_humans_aliens_space_travel_and_physics.csv\"\n",
    "\n",
    "\n",
    "with open(f\"{TRANSCRIPT_PATH}{TRANSCRIPT_FILE_NAME}\") as oldfile, open(f\"{NEW_TRANSCRIPT_FILE_NAME}\", 'w') as newfile:\n",
    "    old_lines = oldfile.read().split('\\n')\n",
    "    clean_lines = [line for line in old_lines if line not in ['', 'WEBVTT']]\n",
    "\n",
    "    for line_idx in range(0, len(clean_lines)-1, 2):\n",
    "         timestamp = clean_lines[line_idx].split('-->')[0].strip()\n",
    "         text = clean_lines[line_idx+1].rstrip()\n",
    "         new_line = f\"{timestamp};{text}\\n\"\n",
    "         newfile.write(new_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamps</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00.000</td>\n",
       "      <td>The following is a conversation with Michio Kaku.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:02.800</td>\n",
       "      <td>He's a theoretical physicist, futurist,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:05.120</td>\n",
       "      <td>and professor at the City College of New York.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:08.360</td>\n",
       "      <td>He's the author of many fascinating books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:10.760</td>\n",
       "      <td>that explore the nature of our reality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00:12.840</td>\n",
       "      <td>and the future of our civilization.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00:15.520</td>\n",
       "      <td>They include Einstein's Cosmos, Physics of the Impossible,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00:19.200</td>\n",
       "      <td>Future of the Mind, Parallel Worlds,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00:21.600</td>\n",
       "      <td>and his latest, The Future of Humanity,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00:24.240</td>\n",
       "      <td>Terraforming Mars Interstellar Travel,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>00:26.640</td>\n",
       "      <td>Immortality, and Our Destiny Beyond Earth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00:29.960</td>\n",
       "      <td>I think it's beautiful and important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00:32.960</td>\n",
       "      <td>when a scientific mind can fearlessly explore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00:35.760</td>\n",
       "      <td>through conversation subjects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00:37.600</td>\n",
       "      <td>just outside of our understanding.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00:40.200</td>\n",
       "      <td>That, to me, is where artificial intelligence is today,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00:43.440</td>\n",
       "      <td>just outside of our understanding,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00:45.680</td>\n",
       "      <td>a place we have to reach for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00:47.440</td>\n",
       "      <td>if we're to uncover the mysteries of the human mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00:50.160</td>\n",
       "      <td>and build human level and superhuman level AI systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>00:53.880</td>\n",
       "      <td>that transform our world for the better.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>00:56.640</td>\n",
       "      <td>This is the Artificial Intelligence Podcast.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>00:59.240</td>\n",
       "      <td>If you enjoy it, subscribe on YouTube,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>01:01.600</td>\n",
       "      <td>give it five stars on iTunes, support it on Patreon,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>01:04.640</td>\n",
       "      <td>or simply connect with me on Twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamps                                                         text\n",
       "0   00:00.000            The following is a conversation with Michio Kaku.\n",
       "1   00:02.800                      He's a theoretical physicist, futurist,\n",
       "2   00:05.120               and professor at the City College of New York.\n",
       "3   00:08.360                    He's the author of many fascinating books\n",
       "4   00:10.760                       that explore the nature of our reality\n",
       "5   00:12.840                          and the future of our civilization.\n",
       "6   00:15.520   They include Einstein's Cosmos, Physics of the Impossible,\n",
       "7   00:19.200                         Future of the Mind, Parallel Worlds,\n",
       "8   00:21.600                      and his latest, The Future of Humanity,\n",
       "9   00:24.240                       Terraforming Mars Interstellar Travel,\n",
       "10  00:26.640                   Immortality, and Our Destiny Beyond Earth.\n",
       "11  00:29.960                         I think it's beautiful and important\n",
       "12  00:32.960                when a scientific mind can fearlessly explore\n",
       "13  00:35.760                                through conversation subjects\n",
       "14  00:37.600                           just outside of our understanding.\n",
       "15  00:40.200      That, to me, is where artificial intelligence is today,\n",
       "16  00:43.440                           just outside of our understanding,\n",
       "17  00:45.680                                 a place we have to reach for\n",
       "18  00:47.440          if we're to uncover the mysteries of the human mind\n",
       "19  00:50.160        and build human level and superhuman level AI systems\n",
       "20  00:53.880                     that transform our world for the better.\n",
       "21  00:56.640                 This is the Artificial Intelligence Podcast.\n",
       "22  00:59.240                       If you enjoy it, subscribe on YouTube,\n",
       "23  01:01.600         give it five stars on iTunes, support it on Patreon,\n",
       "24  01:04.640                         or simply connect with me on Twitter"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_df = pd.read_csv(f\"{NEW_TRANSCRIPT_FILE_NAME}\"\n",
    "                            ,sep=';', \n",
    "                            header=None, names=['timestamps', 'text'],\n",
    "                            parse_dates=['timestamps'], date_format='%H:%M:%S.%f')\n",
    "\n",
    "transcript_df.head(25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ideas\n",
    "* Summarization of main topics in the audio\n",
    "* Go to the mentions of the topics\n",
    "* translation to arabic\n",
    "* The app ask me questions about the text (for language learning) and create a discussion\n",
    "* Overall sentiment in the text\n",
    "* Webapp or Mobile app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since Whisper has not skipped the punctuations in the transcript, we can reconstruct full sentences, ensuring that each sentence is ended with a period `.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the dataframe with full sentences\n",
    "full_transcript_df = pd.DataFrame(columns=transcript_df.columns)\n",
    "\n",
    "for idx, ts, text in transcript_df.itertuples():\n",
    "    while text[-1] != '.':\n",
    "        idx += 1\n",
    "        text += transcript_df.loc[idx]['text']\n",
    "    full_transcript_df = pd.concat([full_transcript_df, pd.DataFrame({'timestamps': ts, 'text': text}, index=[0])], ignore_index=True)\n",
    "\n",
    "# Remove any piece of text if it is included in previous text\n",
    "not_part_of_previous = [True]\n",
    "for i in range(1, len(full_transcript_df)):\n",
    "    not_part_of_previous.append(full_transcript_df['text'][i] not in full_transcript_df['text'][i-1])\n",
    "\n",
    "full_transcript_df = full_transcript_df[not_part_of_previous] \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NER\n",
    "* Add column for persons, orginazations, books, companies, countries, places."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Books\n",
    "Why identifying book titles can be difficult?\n",
    "* The book may contain persons names which are not authors.\n",
    "* The book titles are difficult to identify as such in general. For example \"the Republic\" might or might not be about the book, and if the only indication the model can use is the capitalization it's probably going to make some errors.\n",
    "\n",
    "To be clear, I think it could work to some extent but it would probably make quite a lot of errors.\n",
    "\n",
    "* On the other hand you could obtain a database of books, for instance from Wikipedia (there might be better resources), and you could use this in two ways:\n",
    "\n",
    "1. Directly identify the books/authors in the documents by simple string matching. I would imagine that even if the coverage of the resource is not perfect, this method would easily catch a majority of occurrences.\n",
    "2. In case the above method is not sufficient, it provides you with some good training data from which you could train a NER model in order collect titles which don't exist in the database. Note that there might be issues due to the unknown books being labelled as negative in the training data, so ideally you would have to go manually through the training data and annotate the remaining cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# %pip install -U spaCy\n",
    "# %pip install 'spacy[transformers]'\n",
    "#!python -m spacy download en_core_web_trf # download best-matching version of specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('transformer',\n",
       "  <spacy_transformers.pipeline_component.Transformer at 0x1d1cee402e0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1d1cee40040>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1d1cee953f0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1d1d954e500>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1d1c7cb97c0>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1d1c7caab90>)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "# load a pipeline package by name and return nlp object\n",
    "nlp = spacy.load(\"en_core_web_trf\") #disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\", \"ner\"]\n",
    "\n",
    "# check processing pipeline components of nlp object\n",
    "nlp.pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities:  ['Collapse']\n"
     ]
    }
   ],
   "source": [
    "# create a Doc by processing a string of text with the nlp object\n",
    "doc = nlp(\"So I'm with Jared Diamond, you know, in the book Collapse, \\\n",
    "          where he points out studying the collapse of major civilizations, \\\n",
    "          that it often happens right after things appear to never have been better. Hmm.\")\n",
    "\n",
    "\n",
    "# iterate over tokens in a Doc\n",
    "print(\"Entities: \", [e.text for e in doc.ents if e.label_ == 'WORK_OF_ART'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find book related sentences in the transcript\n",
    "book_related_phrases = [\n",
    "    \"book\", \"books\", \"i read\", \n",
    "    \"everyone should read\", \"you should read\", \"he wrote a novel\",\n",
    "    \"i recommend\", \"highly recommend\", \"you must read\", \n",
    "    \"shouldn't miss\", \"top books\", \"best books\", \n",
    "    \"favorite book\", \"my favorite books\", \"book you need to read\",\n",
    "    \"books to read before\", \"essential books\", \"great book for\",\n",
    "    \"worthy read\", \"book of the year\", \"award winning book\"\n",
    "]\n",
    "\n",
    "\n",
    "def contains_book_phrase(text):\n",
    "    return any(phrase in text.lower() for phrase in book_related_phrases)\n",
    "\n",
    "full_transcript_df[\"is_book_related\"] = full_transcript_df[\"text\"].apply(contains_book_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_titles_candidates(text):\n",
    "    doc = nlp(text)\n",
    "    book_titles_candidates = [e.text for e in doc.ents if e.label_ == 'WORK_OF_ART']\n",
    "    return book_titles_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3                               []\n",
       "182                             []\n",
       "341                             []\n",
       "478       [The Future of the Mind]\n",
       "638                             []\n",
       "859                             []\n",
       "940                             []\n",
       "954     [The Theory of Everything]\n",
       "957                             []\n",
       "1149          [Future of Humanity]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_transcript_df.query(\"is_book_related == True\")[\"text\"].apply(get_book_titles_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'The Future of Humanity',\n",
       " 'subtitle': 'Terraforming Mars, Interstellar Travel, Immortality, and Our Destiny Beyond',\n",
       " 'authors': ['Michio Kaku'],\n",
       " 'publisher': 'Penguin UK',\n",
       " 'publishedDate': '2018-02-28',\n",
       " 'description': 'Human civilization is on the verge of spreading beyond Earth. More than a possibility, it is becoming a necessity: whether our hand is forced by climate change and resource depletion or whether future catastrophes compel us to abandon Earth, one day we will make our homes among the stars. World-renowned physicist and futurist Michio Kaku explores in rich, accessible detail how humanity might gradually develop a sustainable civilization in outer space. With his trademark storytelling verve, Kaku shows us how science fiction is becoming reality: mind-boggling developments in robotics, nanotechnology, and biotechnology could enable us to build habitable cities on Mars; nearby stars might be reached by microscopic spaceships sailing through space on laser beams; and technology might one day allow us to transcend our physical bodies entirely. With irrepressible enthusiasm and wonder, Dr. Kaku takes readers on a fascinating journey to a future in which humanity could finally fulfil its long-awaited destiny among the stars - and perhaps even achieve immortality.',\n",
       " 'industryIdentifiers': [{'type': 'ISBN_13', 'identifier': '9780141986050'},\n",
       "  {'type': 'ISBN_10', 'identifier': '0141986050'}],\n",
       " 'readingModes': {'text': True, 'image': False},\n",
       " 'pageCount': 336,\n",
       " 'printType': 'BOOK',\n",
       " 'categories': ['Science'],\n",
       " 'averageRating': 3.5,\n",
       " 'ratingsCount': 9,\n",
       " 'maturityRating': 'NOT_MATURE',\n",
       " 'allowAnonLogging': True,\n",
       " 'contentVersion': '1.6.7.0.preview.2',\n",
       " 'panelizationSummary': {'containsEpubBubbles': False,\n",
       "  'containsImageBubbles': False},\n",
       " 'imageLinks': {'smallThumbnail': 'http://books.google.com/books/content?id=fSw6DwAAQBAJ&printsec=frontcover&img=1&zoom=5&edge=curl&source=gbs_api',\n",
       "  'thumbnail': 'http://books.google.com/books/content?id=fSw6DwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api'},\n",
       " 'language': 'en',\n",
       " 'previewLink': 'http://books.google.de/books?id=fSw6DwAAQBAJ&printsec=frontcover&dq=Future_of_Humanity&hl=&cd=1&source=gbs_api',\n",
       " 'infoLink': 'https://play.google.com/store/books/details?id=fSw6DwAAQBAJ&source=gbs_api',\n",
       " 'canonicalVolumeLink': 'https://play.google.com/store/books/details?id=fSw6DwAAQBAJ'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_book_info(title):\n",
    "    response = requests.get(f\"https://www.googleapis.com/books/v1/volumes?q={title}\")\n",
    "    data = response.json()\n",
    "    if 'items' in data:\n",
    "        # Just return the first book found\n",
    "        book = data['items'][0]['volumeInfo']\n",
    "        return book\n",
    "\n",
    "get_book_info(\"Future of Humanity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
